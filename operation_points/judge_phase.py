import pandas as pd
import os
import numpy as np

# --- 1. å®šä¹‰å¸¸é‡å’Œè·¯å¾„ ---
BASE_PATH = r"F:\Learning_journal_at_CUHK\FTEC5520_Appl Blockchain & Cryptocur\Simulation\operation_points\data"

# è¾“å…¥æ–‡ä»¶
HF_FILE = os.path.join(BASE_PATH, "weighted_hf_t1_t3.csv")
R1_FILE = os.path.join(BASE_PATH, "r1_hourly_aligned.csv")

# è¾“å‡ºæ–‡ä»¶
OUTPUT_FILE = os.path.join(BASE_PATH, "risk_classification_per_timestamp.csv")

# --- 2. å®šä¹‰é£é™©é˜ˆå€¼ ---
# å›ºå®šé˜ˆå€¼
R1_THRESHOLD = 0.63
LIQUIDATION_LIMIT = 1.0

# åŠ¨æ€åˆ†ä½æ•°é˜ˆå€¼
HF_STATIC_QUANTILE = 0.30  # (q30) é™æ€æ°´å¹³é˜ˆå€¼
HF_VELOCITY_QUANTILE = 0.20 # (q20) åŠ¨æ€é€Ÿåº¦é˜ˆå€¼ (æ•æ‰ä¸‹é™)

# R1 æ–‡ä»¶ä¸­çš„åˆ—å
R1_COLUMN_NAME = 'R1' 


def classify_risk(row, hf_static_thresh, hf_vel_thresh, r1_thresh, liq_limit):
    """
    æ ¹æ®æ–°çš„ä¸‰ç»´é£é™©æ¨¡å‹ï¼ˆR1, HFæ°´å¹³, HFé€Ÿåº¦ï¼‰åº”ç”¨é£é™©åˆ†ç±»ã€‚
    """
    # 1. å®šä¹‰æ‰€æœ‰é£é™©æ¡ä»¶
    hf_is_low = row['Weighted_HF'] < hf_static_thresh
    hf_is_dropping = row['HF_pct_change'] < hf_vel_thresh
    r1_is_high = row[R1_COLUMN_NAME] > r1_thresh
    hf_is_liquidated = row['Weighted_HF'] < liq_limit

    # 2. æŒ‰ä¼˜å…ˆçº§åˆ†é…é£é™©
    
    # ğŸ”´ é«˜é£é™© (High Risk)
    if hf_is_liquidated:
        quadrant = "High_Personal_Risk_Liquidated (HF < 1.0)"
        level = "High Risk"
    elif r1_is_high and hf_is_dropping:
        quadrant = "High_System_Risk_AND_HF_Dropping"
        level = "High Risk"
        
    # ğŸŸ  ä¸­é£é™© (Medium Risk)
    elif r1_is_high:
        quadrant = "High_System_Risk_Only"
        level = "Medium Risk"
    elif hf_is_dropping:
        quadrant = "HF_Dropping_Only"
        level = "Medium Risk"
    elif hf_is_low:
        quadrant = "Low_Static_HF_Only"
        level = "Medium Risk"
        
    # ğŸŸ¢ ä½é£é™© (Low Risk)
    else:
        quadrant = "Low_Risk_All_Stable"
        level = "Low Risk"
        
    return pd.Series([quadrant, level])

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼šåŠ è½½ã€åˆå¹¶ã€è®¡ç®—æŒ‡æ ‡å’Œé˜ˆå€¼ã€åˆ†ç±»å’Œä¿å­˜ã€‚
    """
    print("--- é£é™©è±¡é™åˆ’åˆ†è„šæœ¬ (åŠ¨æ€ä¸‰ç»´æ¨¡å‹) ---")
    
    # --- 1. åŠ è½½æ•°æ® ---
    try:
        print(f"Loading Weighted HF data from: {HF_FILE}")
        df_hf = pd.read_csv(HF_FILE)
        df_hf['datetime_utc'] = pd.to_datetime(df_hf['datetime_utc'])
    except FileNotFoundError:
        print(f"  [Error] æ–‡ä»¶æœªæ‰¾åˆ°: {HF_FILE}")
        return
    except Exception as e:
        print(f"  [Error] åŠ è½½ HF æ–‡ä»¶å¤±è´¥: {e}")
        return

    try:
        print(f"Loading CLEANED R1 data from: {R1_FILE}")
        df_r1 = pd.read_csv(R1_FILE)
        df_r1['datetime_utc'] = pd.to_datetime(df_r1['datetime_utc'])
    except FileNotFoundError:
        print(f"  [Error] æ–‡ä»¶æœªæ‰¾åˆ°: {R1_FILE}")
        return
    except Exception as e:
        print(f"  [Error] åŠ è½½ R1 æ–‡ä»¶å¤±è´¥: {e}")
        return
        
    # --- 2. åˆå¹¶æ•°æ® ---
    print("Merging HF and R1 data on 'datetime_utc'...")
    df_merged = pd.merge(df_hf, df_r1, on='datetime_utc', how='inner')
    
    if df_merged.empty:
        print("  [Error] åˆå¹¶åæ•°æ®ä¸ºç©ºã€‚")
        return
    if R1_COLUMN_NAME not in df_merged.columns:
        print(f"  [Error] R1 æ–‡ä»¶ä¸­æœªæ‰¾åˆ°åˆ—: '{R1_COLUMN_NAME}'ã€‚")
        return
        
    print(f"åˆå¹¶æˆåŠŸã€‚å…±æœ‰ {len(df_merged)} ä¸ªé‡å çš„æ—¶é—´ç‚¹ã€‚")

    # --- 3. (!!!) æ–°å¢: è®¡ç®—åŠ¨æ€æŒ‡æ ‡å’Œé˜ˆå€¼ ---
    
    # a. è®¡ç®— HF 1å°æ—¶ç™¾åˆ†æ¯”å˜åŒ–
    df_merged['HF_pct_change'] = df_merged['Weighted_HF'].pct_change()
    # å¡«å……ç¬¬ä¸€ä¸ª NaN å€¼ä¸º 0 (ä»£è¡¨æ— å˜åŒ–)
    df_merged['HF_pct_change'] = df_merged['HF_pct_change'].fillna(0.0)

    # b. è®¡ç®—åŠ¨æ€é˜ˆå€¼
    HF_STATIC_THRESHOLD = df_merged['Weighted_HF'].quantile(HF_STATIC_QUANTILE)
    HF_VELOCITY_THRESHOLD = df_merged['HF_pct_change'].quantile(HF_VELOCITY_QUANTILE)
    
    print("\n--- é£é™©é˜ˆå€¼å®šä¹‰ (åŠ¨æ€æ¨¡å‹) ---")
    print(f"ç³»ç»Ÿé£é™© (R1) é˜ˆå€¼: > {R1_THRESHOLD}")
    print(f"ç»å¯¹æ¸…ç®— (HF) é˜ˆå€¼: < {LIQUIDATION_LIMIT}")
    print(f"é™æ€HF (q{int(HF_STATIC_QUANTILE*100)}) é˜ˆå€¼: < {HF_STATIC_THRESHOLD:.6f}")
    print(f"åŠ¨æ€HF (q{int(HF_VELOCITY_QUANTILE*100)}) é˜ˆå€¼: < {HF_VELOCITY_THRESHOLD:.6f} (å³ä¸‹é™è¶…è¿‡ {abs(HF_VELOCITY_THRESHOLD*100):.3f}%)")

    # --- 4. åº”ç”¨åˆ†ç±» ---
    print("\nApplying classification to each timestamp...")
    
    df_merged[['Quadrant', 'Risk_Level']] = df_merged.apply(
        classify_risk, 
        axis=1, 
        hf_static_thresh=HF_STATIC_THRESHOLD, 
        hf_vel_thresh=HF_VELOCITY_THRESHOLD,
        r1_thresh=R1_THRESHOLD,
        liq_limit=LIQUIDATION_LIMIT
    )
    
    print("Classification complete.")
    
    # æ£€æŸ¥é£é™©äº‹ä»¶çš„åˆ†å¸ƒ
    print(f"\n--- ç»“æœ: é£é™©ç­‰çº§åˆ†å¸ƒ ---")
    risk_distribution = df_merged['Risk_Level'].value_counts()
    print(risk_distribution)
    
    if "High Risk" not in risk_distribution:
        print("\n(æç¤º: ä»æœªæ‰¾åˆ°é«˜é£é™©äº‹ä»¶ã€‚è¿™è¡¨æ˜ç³»ç»Ÿè„†å¼± (R1 > 0.63) æ—¶ï¼Œ")
        print(" HF ä»æœªåŒæ—¶ç»å†å¿«é€Ÿä¸‹é™ (ä½äº q20)ã€‚è¿™æ˜¯ä¸€ä¸ªçœŸå®çš„æ•°æ®å‘ç°ã€‚)")


    # --- 5. ä¿å­˜ç»“æœ ---
    columns_order = [
        'datetime_utc', 
        'Risk_Level', 
        'Quadrant', 
        'Weighted_HF', 
        'HF_pct_change',
        R1_COLUMN_NAME
    ]
    other_cols = [col for col in df_merged.columns if col not in columns_order]
    final_df = df_merged[columns_order + other_cols]
    
    try:
        final_df.to_csv(OUTPUT_FILE, index=False, float_format='%.18f')
        print(f"\nSuccessfully saved risk classification to:\n{OUTPUT_FILE}")
    except Exception as e:
        print(f"\n[Error] Failed to save output file: {e}")


# --- è„šæœ¬å…¥å£ ---
if __name__ == "__main__":
    main()